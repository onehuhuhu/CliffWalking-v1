# 🧗 CliffWalking-v1 强化学习算法对比与演示

欢迎来到 **CliffWalking (悬崖漫步)** 强化学习项目！本项目旨在通过经典的悬崖环境，直观地对比 **Q-Learning** 与 **SARSA** 两种经典算法的学习效果与策略差异。

## 📂 项目文件概览

| **文件名**              | **类型**       | **描述**                                                  |
| ----------------------- | -------------- | --------------------------------------------------------- |
| `train_all.py`          | 🧠 **训练大脑** | 负责训练模型，生成 Q 表和历史记录，并保存为 `.npy` 文件。 |
| `replay_compare_pro.py` | 📺 **回放中心** | 加载训练数据，可视化智能体的行动，支持实时交互控制。      |
| `*.npy`                 | 💾 **数据存档** | 训练后自动生成的权重文件（运行训练脚本后出现）。          |

------

## 🛠️ 环境依赖

在开始之前，请确保你的 Python 环境中安装了以下库：

```
pip install gymnasium numpy pygame
```

*(注意：`gymnasium` 用于构建环境，`pygame` 用于检测键盘事件和渲染窗口)*

------

## 🚀 快速开始 (Quick Start)

### 第一步：训练模型 (Training) 🏋️‍♂️

首先，我们需要让智能体“上学”去探索悬崖。运行以下命令：

```
python train_all.py
```

**运行效果：**

- 脚本会自动训练 **Q-Learning** 和 **SARSA** 算法各 500 轮。
- 控制台会打印训练进度：`Episode 100/500 完成...`
- 完成后，目录下会生成 4 个 `.npy` 数据文件。✅

### 第二步：可视化回放 (Replay) 🎮

训练完成后，就可以欣赏智能体的表演了！运行：

```
python replay_compare_pro.py
```

**交互指南：**

1. **选择算法**：
   - 输入 `1` 👉 查看 **Q-Learning** (通常更勇敢，贴着悬崖走最优路径)。
   - 输入 `2` 👉 查看 **SARSA** (通常更保守，为了安全会绕远路)。
   - 输入 `Q` 👉 退出程序。
2. **选择展示内容**：
   - 输入 **数字 (0-499)**：回放训练过程中某一具体的轮次（看看它是怎么一点点变聪明的）。
   - 输入 **`best`**：🌟 **强力推荐**！直接展示当前 Q 表下的**最优策略**路径。
3. **观看与控制**：
   - 窗口弹出，智能体开始移动。
   - 🛑 **紧急停止**：如果回放太长不想看了，直接按键盘上的 **`Esc` 键** 或点击窗口的 **红叉**，即可立即中止当前回放并返回主菜单。

------

## 🧐 算法差异观察点

当你运行 `best` 模式时，请留意以下有趣的现象：

- Q-Learning (大胆派) 😈：

  它倾向于走在悬崖的最边缘，因为这是理论上的最短路径。一旦有风吹草动（Epsilon探索），它就很容易掉下去。

- SARSA (稳健派) 🛡️：

  因为它在更新时考虑了下一步的探索风险，所以它通常会选择远离悬崖边缘的“安全路线”，哪怕路程稍微远一点。

------

## 📝 常见问题 (FAQ)

Q: 运行 replay 时报错 "FileNotFoundError"？

A: 你一定还没运行训练脚本！请先执行 python train_all.py 生成数据文件。🚫

Q: 按了 Esc 没反应？

A: 请确保你的输入焦点在 Pygame 的图形窗口 上，而不是在终端命令行里。🖱️

------

祝你玩得开心！如果有新的想法，随时可以修改代码来测试！ 🤖✨