# 🧗 CliffWalking-v1 强化学习算法对比与演示

欢迎来到 **CliffWalking (悬崖漫步)** 强化学习项目！本项目旨在通过经典的悬崖环境，直观地对比 **Q-Learning** 与 **SARSA** 两种经典算法的学习效果与策略差异。

## 📂 项目文件概览

| **文件名**                    | **类型**         | **描述**                                                     |
| ----------------------------- | ---------------- | ------------------------------------------------------------ |
| `cliff_walking_comparison.py` | 📊 **数据分析师** | **(新!)** 一键运行训练，直接画出**奖励收敛曲线**和**最终路径图**，适合写报告或分析算法性能。 |
| `train_all.py`                | 🧠 **训练大脑**   | 负责为交互回放模式训练模型，生成 `.npy` 存档文件。           |
| `replay_compare_pro.py`       | 📺 **回放中心**   | 读取训练存档，用动画窗口演示智能体是如何一步步走的。         |

## 🛠️ 环境依赖

在开始之前，请确保你的 Python 环境中安装了以下库：

```
pip install gymnasium numpy pygame matplotlib
```

*(注意：`gymnasium` 用于构建环境，`pygame` 用于检测键盘事件和渲染窗口)*

------

## 一键分析与绘图  📈

如果你想看**训练曲线**（模型学得快不快）以及**最终效果图**（到底走的哪条路），请运行：

```
python cliff_walking_comparison.py
```

**运行后你将看到：**

1. **控制台输出**：实时打印训练进度和 Epsilon 衰减情况。
2. **弹窗图表 1 (曲线图)**：展示 Q-Learning 和 SARSA 在 500 轮训练中的 Reward 变化。你可以清晰地看到它们是如何从“一直掉悬崖”变成“安全到达”的。
3. **弹窗图表 2 & 3 (路径图)**：
   - 🔵 **Q-Learning**：你会发现它贴着悬崖边走（路径最短，但在训练中风险大）。
   - 🔵 **SARSA**：你会发现它离悬崖远远的（路径较长，但更安全）。

## 交互式动画回放 🎮

### **第一步：训练模型 (Training) 🏋️‍♂️**

首先，我们需要让智能体“上学”去探索悬崖。运行以下命令：

```
python train_all.py
```

**运行效果：**

- 脚本会自动训练 **Q-Learning** 和 **SARSA** 算法各 500 轮。
- 控制台会打印训练进度：`Episode 100/500 完成...`
- 完成后，目录下会生成 4 个 `.npy` 数据文件。✅

### **第二步：可视化回放 (Replay) 🎮**

训练完成后，就可以欣赏智能体的表演了！运行：

```
python replay_compare_pro.py
```

**交互指南：**

1. **选择算法**：
   - 输入 `1` 👉 查看 **Q-Learning** (通常更勇敢，贴着悬崖走最优路径)。
   - 输入 `2` 👉 查看 **SARSA** (通常更保守，为了安全会绕远路)。
   - 输入 `Q` 👉 退出程序。
2. **选择展示内容**：
   - 输入 **数字 (0-499)**：回放训练过程中某一具体的轮次（看看它是怎么一点点变聪明的）。
   - 输入 **`best`**：🌟 **强力推荐**！直接展示当前 Q 表下的**最优策略**路径。
3. **观看与控制**：
   - 窗口弹出，智能体开始移动。
   - 🛑 **紧急停止**：如果回放太长不想看了，直接按键盘上的 **`Esc` 键** 或点击窗口的 **红叉**，即可立即中止当前回放并返回主菜单。

------

## 🧐 算法差异观察点

当你运行 `best` 模式时，请留意以下有趣的现象：

- Q-Learning (大胆派) 😈：

  它倾向于走在悬崖的最边缘，因为这是理论上的最短路径。一旦有风吹草动（Epsilon探索），它就很容易掉下去。

- SARSA (稳健派) 🛡️：

  因为它在更新时考虑了下一步的探索风险，所以它通常会选择远离悬崖边缘的“安全路线”，哪怕路程稍微远一点。

------

## 📝 常见问题 (FAQ)

Q: 运行 replay 时报错 "FileNotFoundError"？

A: 你一定还没运行训练脚本！请先执行 python train_all.py 生成数据文件。🚫

Q: 按了 Esc 没反应？

A: 请确保你的输入焦点在 Pygame 的图形窗口 上，而不是在终端命令行里。🖱️

Q: 图表窗口弹出来后程序不动了？

A: matplotlib 的绘图窗口会阻塞程序。你需要关闭当前的图表窗口，程序才会继续弹出下一张图或结束运行。❌

------

祝你玩得开心！如果有新的想法，随时可以修改代码来测试！ 🤖✨

（友情提示：此项目代码文档均为Gemini 3Pro生成，仅供学习参考。）